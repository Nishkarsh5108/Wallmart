unemployement grants food voucheras which are used in wallmarts 
store number is important: big store != small store and rich area != small area
date: year end, festival season 
holiday flag : it is holoday so sales might spike
temp: people buy different stuff at different
fuel price: people drive less, fewer store visits: matters more in US because distance is more berween stores and residentails a car is a life support, also fuel is a unavoidable expense so, less money left at the end of the month to spend on wallmart 
CPI: measure of inflation, prices go boom if inflation high, but sales might increase, cause stuff is more expensive rn, even though less people might be buying it but overall sales amount can increase
CPI = (cost of basket now / cost of basket in base year) *100  [basket here is the stuff people usually buy]





handleing store number:

one hot encoding: binary classification of if it is that store or not : weights would get assigned to each store column, and as it gets 1 we use it: will help in providing a base value for the sales of the store and the rest parameters would increase or decrease the sales 

target/mean encoding: would assign average value of sales of that store (from training data), so would give the "base value" (which i assume would be calculated by weights in one hot encoding) 

embedding encoding: learn a dense vector like [0.21, -0.03, 0.88] which would describe some stuff about the store 


final verdict: 
embeddings look cool, and would be more of use when we have more stores, but since there are only 45 stores in the data, embeddings would overfit

one hot encoding will create a lot of mess by making 45 extra sparse columns 

target/mean encoding would be a sweet spot, we would make sure to do:
out of fold encoding
combine lag features
i am thinking of using are last week's sales and average of last 4 week's sales so what i plan is creating the lag features of sales then doing the train test split, and then after the split i will get store mean values (calculated from only training data, and using the same training means in testing)

for train test split i would use rolling validation, something like this: 
Train: 2010–2011  → Test: early 2012
Train: 2010–2012  → Test: mid 2012






handeling DATE and holiday flag: (adding feature) will do it before train test split beacuse no leakeage would occur
year
season: four seasons are there in USA (spring, summer, fall, winter)
week of year 
is year end(end year vacations)
is year mid (mid year vacations)
time_idx (weeks since start of data)
weeks to next holiday
weeks since last holiday
unemployment*holiday_flag





handeling temprature (seasonal mean and SD of the temprature)
temp_zscore (temprature - mean) / SD
abnormally_hot (if temp_zscore > 1) (or 2 )
abnormally_cold (if temp_zscore < -1) (or -2)


fuel price:
sudden change in fuel price 
we can make something like is fuel price is high during the end of month 
fuel price * unemployment

CPI: 
rolling CPI trend over 4 weeks (trend_4w = cpi.diff(1).rolling(4).mean())
cpi * unemployment
CPI*time_idx

unemployment:
due to SNAP (supplemental nutrition assistance program) money in EBT cards,  people buy more essential items from wallmart, so sales might increase, but it is the daily need wale items so sales might not increase that much as the high value items(electronics, etc) sales drop 
new features are mentioned above
